{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Course 1 - Part 6 - Lesson 2 - Notebook.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JamesMcGuigan/dlaicourse/blob/master/Course%201%20-%20Part%206%20-%20Lesson%202%20-%20Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6gHiH-I7uFa",
        "colab_type": "text"
      },
      "source": [
        "#Improving Computer Vision Accuracy using Convolutions\n",
        "\n",
        "In the previous lessons you saw how to do fashion recognition using a Deep Neural Network (DNN) containing three layers -- the input layer (in the shape of the data), the output layer (in the shape of the desired output) and a hidden layer. You experimented with the impact of different sized of hidden layer, number of training epochs etc on the final accuracy.\n",
        "\n",
        "For convenience, here's the entire code again. Run it and take a note of the test accuracy that is printed out at the end. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcsRtq9OLorS",
        "colab_type": "code",
        "outputId": "b7c47c12-8932-4dba-f61f-a0215d6fa47b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images / 255.0\n",
        "test_images=test_images / 255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(\n",
        "    optimizer='adam', \n",
        "    loss='sparse_categorical_crossentropy', \n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.summary()\n",
        "model.fit(\n",
        "    training_images, training_labels, \n",
        "    validation_data=(test_images, test_labels),\n",
        "    epochs=20,\n",
        ")\n",
        "print('----- test_loss')\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "flatten_4 (Flatten)          (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 128)               100480    \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 101,770\n",
            "Trainable params: 101,770\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 0.5008 - acc: 0.8243 - val_loss: 0.4561 - val_acc: 0.8373\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 4s 73us/sample - loss: 0.3775 - acc: 0.8636 - val_loss: 0.4006 - val_acc: 0.8578\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 4s 73us/sample - loss: 0.3368 - acc: 0.8759 - val_loss: 0.3721 - val_acc: 0.8697\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.3113 - acc: 0.8858 - val_loss: 0.3468 - val_acc: 0.8749\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.2927 - acc: 0.8917 - val_loss: 0.3580 - val_acc: 0.8703\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 4s 74us/sample - loss: 0.2796 - acc: 0.8960 - val_loss: 0.3348 - val_acc: 0.8765\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 4s 71us/sample - loss: 0.2682 - acc: 0.9010 - val_loss: 0.3497 - val_acc: 0.8780\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 4s 71us/sample - loss: 0.2584 - acc: 0.9036 - val_loss: 0.3544 - val_acc: 0.8761\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.2493 - acc: 0.9058 - val_loss: 0.3421 - val_acc: 0.8796\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.2418 - acc: 0.9086 - val_loss: 0.3525 - val_acc: 0.8774\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 4s 73us/sample - loss: 0.2314 - acc: 0.9136 - val_loss: 0.3440 - val_acc: 0.8809\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 4s 71us/sample - loss: 0.2243 - acc: 0.9150 - val_loss: 0.3437 - val_acc: 0.8796\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 4s 74us/sample - loss: 0.2184 - acc: 0.9174 - val_loss: 0.3306 - val_acc: 0.8862\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.2116 - acc: 0.9208 - val_loss: 0.3291 - val_acc: 0.8872\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.2057 - acc: 0.9219 - val_loss: 0.3402 - val_acc: 0.8840\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 4s 71us/sample - loss: 0.1988 - acc: 0.9253 - val_loss: 0.3539 - val_acc: 0.8800\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 4s 71us/sample - loss: 0.1940 - acc: 0.9275 - val_loss: 0.3545 - val_acc: 0.8838\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 4s 70us/sample - loss: 0.1908 - acc: 0.9274 - val_loss: 0.3500 - val_acc: 0.8878\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 4s 73us/sample - loss: 0.1859 - acc: 0.9305 - val_loss: 0.3544 - val_acc: 0.8881\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 4s 71us/sample - loss: 0.1822 - acc: 0.9316 - val_loss: 0.3623 - val_acc: 0.8860\n",
            "----- test_loss\n",
            "10000/10000 [==============================] - 0s 43us/sample - loss: 0.3623 - acc: 0.8860\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zldEXSsF8Noz",
        "colab_type": "text"
      },
      "source": [
        "Your accuracy is probably about 89% on training and 87% on validation...not bad...But how do you make that even better? One way is to use something called Convolutions. I'm not going to details on Convolutions here, but the ultimate concept is that they narrow down the content of the image to focus on specific, distinct, details. \n",
        "\n",
        "If you've ever done image processing using a filter (like this: https://en.wikipedia.org/wiki/Kernel_(image_processing)) then convolutions will look very familiar.\n",
        "\n",
        "In short, you take an array (usually 3x3 or 5x5) and pass it over the image. By changing the underlying pixels based on the formula within that matrix, you can do things like edge detection. So, for example, if you look at the above link, you'll see a 3x3 that is defined for edge detection where the middle cell is 8, and all of its neighbors are -1. In this case, for each pixel, you would multiply its value by 8, then subtract the value of each neighbor. Do this for every pixel, and you'll end up with a new image that has the edges enhanced.\n",
        "\n",
        "This is perfect for computer vision, because often it's features that can get highlighted like this that distinguish one item for another, and the amount of information needed is then much less...because you'll just train on the highlighted features.\n",
        "\n",
        "That's the concept of Convolutional Neural Networks. Add some layers to do convolution before you have the dense layers, and then the information going to the dense layers is more focussed, and possibly more accurate.\n",
        "\n",
        "Run the below code -- this is the same neural network as earlier, but this time with Convolutional layers added first. It will take longer, but look at the impact on the accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0tFgT1MMKi6",
        "colab_type": "code",
        "outputId": "9e9dc334-d088-4fc3-ad61-8254f51c5ae2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 2 * Conv2D(3, 16)  = Total params:    55,098 | Epoch 11 |  6s 100us/sample - loss: 0.1072 - acc: 0.9585 - val_loss: 0.3485 - val_acc: 0.9019 \n",
        "# 2 * Conv2D(3, 32)  = Total params:   113,386 | Epoch  8 |  6s 101us/sample - loss: 0.1676 - acc: 0.9364 - val_loss: 0.2672 - val_acc: 0.9081\n",
        "# 2 * Conv2D(3, 64)  = Total params:   243,786 | Epoch  8 |  6s 106us/sample - loss: 0.1335 - acc: 0.9493 - val_loss: 0.2688 - val_acc: 0.9152\n",
        "# 2 * Conv2D(3, 128) = Total params:   559,882 | Epoch  8 |  7s 123us/sample - loss: 0.1102 - acc: 0.9583 - val_loss: 0.2836 - val_acc: 0.9128\n",
        "# 2 * Conv2D(3, 256) = Total params: 1,413,258 | Epoch  7 | 10s 162us/sample - loss: 0.1080 - acc: 0.9594 - val_loss: 0.2989 - val_acc: 0.9115\n",
        "\n",
        "# 1 * Conv2D(3, 256) = Total params: 5,541,770 | Epoch  7 |  8s 138us/sample - loss: 0.0808 - acc: 0.9708 - val_loss: 0.3168 - val_acc: 0.9123\n",
        "# 1 * Conv2D(3, 128) = Total params: 2,771,594 | Epoch  7 |  7s 113us/sample - loss: 0.0895 - acc: 0.9659 - val_loss: 0.2907 - val_acc: 0.9147\n",
        "# 1 * Conv2D(5, 128) = Total params: 2,364,042 | Epoch  8 |  7s 123us/sample - loss: 0.0878 - acc: 0.9670 - val_loss: 0.3187 - val_acc: 0.9072\n",
        "\n",
        "\n",
        "# 3 * Conv2D(3, 128, valid) + 2*MaxPool() = Total params:   445,322 | Epoch  8 |  8s 132us/sample - loss: 0.1236 - acc: 0.9533 - val_loss: 0.3006 - val_acc: 0.9064\n",
        "# 3 * Conv2D(3, 128, same)  + 2*MaxPool() = Total params: 1,100,682 | Epoch  6 |  9s 143us/sample - loss: 0.1095 - acc: 0.9589 - val_loss: 0.2557 - val_acc: 0.9207\n",
        "# 3 * Conv2D(3, 128, same)  + 3*MaxPool() = Total params:   445,322 | Epoch  8 |  9s 143us/sample - loss: 0.1040 - acc: 0.9609 - val_loss: 0.2593 - val_acc: 0.9136\n",
        "# 3 * Conv2D(3, 256, same)  + 2*MaxPool() = Total params: 2,789,770 | Epoch  8 | 13s 212us/sample - loss: 0.0676 - acc: 0.9745 - val_loss: 0.3377 - val_acc: 0.9150\n",
        "\n",
        "# 3 * Conv2D(5, 128, same)  + 2*MaxPool()    = Total params:  1,627,018 | Epoch  6 | 11s 179us/sample - loss: 0.1246 - acc: 0.9519 - val_loss: 0.2649 - val_acc: 0.9180\n",
        "# 3 * Conv2D(5,  64, same)  + 2*MaxPool()    = Total params:    609,418 | Epoch  7 |  8s 136us/sample - loss: 0.1170 - acc: 0.9553 - val_loss: 0.2654 - val_acc: 0.9156\n",
        "# Conv2D(5,64,same) + 2*Conv2D(3,64,same)    = Total params:    478,346 | Epoch  7 |  8s 126us/sample - loss: 0.1194 - acc: 0.9550 - val_loss: 0.2633 - val_acc: 0.9190\n",
        "# 3 * Conv2D(7/5/3, 128, same) + 2*MaxPool() = Total params:  1,367,946 | Epoch  8 | 10s 171us/sample - loss: 0.1037 - acc: 0.9599 - val_loss: 0.2948 - val_acc: 0.9159 - patience=3 (default)\n",
        "# 3 * Conv2D(7/5/3, 128, same) + 2*MaxPool() = Total params:  1,367,946 | Epoch 14 | 10s 175us/sample - loss: 0.0524 - acc: 0.9804 - val_loss: 0.4592 - val_acc: 0.9170 - patience=10\n",
        "# 3 * Conv2D(7/5/3, 256, same) + 2*MaxPool() = Total params:  3,848,586 | Epoch  8 | 17s 284us/sample - loss: 0.0992 - acc: 0.9620 - val_loss: 0.2987 - val_acc: 0.9160\n",
        "# 3 * Conv2D(7/5/3, 512, same) + 2*MaxPool() = Total params: 12,152,202 | Epoch  8 | 38s 638us/sample - loss: 0.0956 - acc: 0.9638 - val_loss: 0.2840 - val_acc: 0.9138\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow_core.python.keras.callbacks import EarlyStopping\n",
        "\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(128, (7,7), activation='relu', padding='same', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(128, (5,5), activation='relu', padding='same',),\n",
        "  tf.keras.layers.MaxPooling2D(2,2),\n",
        "  tf.keras.layers.Conv2D(128, (3,3), activation='relu', padding='same',),  \n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(\n",
        "    optimizer='adam', \n",
        "    loss='sparse_categorical_crossentropy', \n",
        "    metrics=['accuracy']\n",
        ")\n",
        "model.summary()\n",
        "model.fit(\n",
        "    training_images, training_labels, \n",
        "    validation_data=(test_images, test_labels),\n",
        "    epochs=99,\n",
        "    callbacks = [\n",
        "      EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3),                              \n",
        "      EarlyStopping(monitor='loss', mode='min', verbose=1, patience=10),             \n",
        "    ]    \n",
        ")\n",
        "\n",
        "print('----- test_loss')\n",
        "test_loss = model.evaluate(test_images, test_labels)\n"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n",
            "Model: \"sequential_46\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_86 (Conv2D)           (None, 28, 28, 128)       6400      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_73 (MaxPooling (None, 14, 14, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_87 (Conv2D)           (None, 14, 14, 128)       409728    \n",
            "_________________________________________________________________\n",
            "max_pooling2d_74 (MaxPooling (None, 7, 7, 128)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_88 (Conv2D)           (None, 7, 7, 128)         147584    \n",
            "_________________________________________________________________\n",
            "flatten_46 (Flatten)         (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_92 (Dense)             (None, 128)               802944    \n",
            "_________________________________________________________________\n",
            "dense_93 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,367,946\n",
            "Trainable params: 1,367,946\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/99\n",
            "60000/60000 [==============================] - 12s 199us/sample - loss: 0.4013 - acc: 0.8526 - val_loss: 0.3180 - val_acc: 0.8805\n",
            "Epoch 2/99\n",
            "60000/60000 [==============================] - 11s 183us/sample - loss: 0.2674 - acc: 0.9002 - val_loss: 0.2867 - val_acc: 0.8980\n",
            "Epoch 3/99\n",
            "60000/60000 [==============================] - 11s 189us/sample - loss: 0.2220 - acc: 0.9169 - val_loss: 0.2603 - val_acc: 0.9045\n",
            "Epoch 4/99\n",
            "60000/60000 [==============================] - 11s 188us/sample - loss: 0.1927 - acc: 0.9271 - val_loss: 0.2606 - val_acc: 0.9064\n",
            "Epoch 5/99\n",
            "60000/60000 [==============================] - 11s 184us/sample - loss: 0.1643 - acc: 0.9369 - val_loss: 0.2761 - val_acc: 0.9074\n",
            "Epoch 6/99\n",
            "60000/60000 [==============================] - 11s 183us/sample - loss: 0.1419 - acc: 0.9456 - val_loss: 0.2673 - val_acc: 0.9143\n",
            "Epoch 7/99\n",
            "60000/60000 [==============================] - 11s 183us/sample - loss: 0.1201 - acc: 0.9545 - val_loss: 0.2873 - val_acc: 0.9166\n",
            "Epoch 8/99\n",
            "60000/60000 [==============================] - 11s 182us/sample - loss: 0.1047 - acc: 0.9606 - val_loss: 0.3121 - val_acc: 0.9122\n",
            "Epoch 9/99\n",
            "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0907 - acc: 0.9658 - val_loss: 0.3431 - val_acc: 0.9194\n",
            "Epoch 10/99\n",
            "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0815 - acc: 0.9687 - val_loss: 0.3585 - val_acc: 0.9150\n",
            "Epoch 11/99\n",
            "60000/60000 [==============================] - 11s 185us/sample - loss: 0.0712 - acc: 0.9731 - val_loss: 0.3894 - val_acc: 0.9159\n",
            "Epoch 12/99\n",
            "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0627 - acc: 0.9767 - val_loss: 0.4278 - val_acc: 0.9150\n",
            "Epoch 13/99\n",
            "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0604 - acc: 0.9782 - val_loss: 0.4998 - val_acc: 0.9156\n",
            "Epoch 14/99\n",
            "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0547 - acc: 0.9798 - val_loss: 0.4690 - val_acc: 0.9102\n",
            "Epoch 15/99\n",
            "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0493 - acc: 0.9824 - val_loss: 0.4969 - val_acc: 0.9157\n",
            "Epoch 16/99\n",
            "60000/60000 [==============================] - 11s 185us/sample - loss: 0.0488 - acc: 0.9826 - val_loss: 0.5178 - val_acc: 0.9128\n",
            "Epoch 17/99\n",
            "60000/60000 [==============================] - 11s 185us/sample - loss: 0.0465 - acc: 0.9836 - val_loss: 0.5601 - val_acc: 0.9119\n",
            "Epoch 18/99\n",
            "60000/60000 [==============================] - 11s 186us/sample - loss: 0.0506 - acc: 0.9835 - val_loss: 0.5012 - val_acc: 0.9114\n",
            "Epoch 19/99\n",
            "60000/60000 [==============================] - 11s 190us/sample - loss: 0.0397 - acc: 0.9863 - val_loss: 0.5507 - val_acc: 0.9111\n",
            "Epoch 20/99\n",
            "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0411 - acc: 0.9859 - val_loss: 0.6445 - val_acc: 0.9153\n",
            "Epoch 21/99\n",
            "60000/60000 [==============================] - 11s 186us/sample - loss: 0.0423 - acc: 0.9859 - val_loss: 0.6687 - val_acc: 0.9137\n",
            "Epoch 22/99\n",
            "60000/60000 [==============================] - 11s 187us/sample - loss: 0.0418 - acc: 0.9866 - val_loss: 0.5933 - val_acc: 0.9080\n",
            "Epoch 23/99\n",
            "60000/60000 [==============================] - 11s 187us/sample - loss: 0.0395 - acc: 0.9871 - val_loss: 0.6970 - val_acc: 0.9191\n",
            "Epoch 24/99\n",
            "60000/60000 [==============================] - 11s 186us/sample - loss: 0.0362 - acc: 0.9875 - val_loss: 0.6703 - val_acc: 0.9158\n",
            "Epoch 25/99\n",
            "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0341 - acc: 0.9888 - val_loss: 0.6965 - val_acc: 0.9156\n",
            "Epoch 26/99\n",
            "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0349 - acc: 0.9887 - val_loss: 0.6472 - val_acc: 0.9145\n",
            "Epoch 27/99\n",
            "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0381 - acc: 0.9880 - val_loss: 0.7749 - val_acc: 0.9151\n",
            "Epoch 28/99\n",
            "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0308 - acc: 0.9900 - val_loss: 0.7333 - val_acc: 0.9110\n",
            "Epoch 29/99\n",
            "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0334 - acc: 0.9898 - val_loss: 0.7265 - val_acc: 0.9091\n",
            "Epoch 30/99\n",
            "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0352 - acc: 0.9889 - val_loss: 0.7566 - val_acc: 0.9095\n",
            "Epoch 31/99\n",
            "60000/60000 [==============================] - 11s 187us/sample - loss: 0.0322 - acc: 0.9904 - val_loss: 0.7906 - val_acc: 0.9133\n",
            "Epoch 32/99\n",
            "60000/60000 [==============================] - 11s 187us/sample - loss: 0.0331 - acc: 0.9899 - val_loss: 0.8405 - val_acc: 0.9148\n",
            "Epoch 33/99\n",
            "60000/60000 [==============================] - 11s 186us/sample - loss: 0.0317 - acc: 0.9904 - val_loss: 0.8626 - val_acc: 0.9128\n",
            "Epoch 34/99\n",
            "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0361 - acc: 0.9902 - val_loss: 0.9199 - val_acc: 0.9105\n",
            "Epoch 35/99\n",
            "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0354 - acc: 0.9900 - val_loss: 0.8726 - val_acc: 0.9104\n",
            "Epoch 36/99\n",
            "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0265 - acc: 0.9925 - val_loss: 1.0221 - val_acc: 0.9147\n",
            "Epoch 37/99\n",
            "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0345 - acc: 0.9902 - val_loss: 1.0438 - val_acc: 0.9105\n",
            "Epoch 38/99\n",
            "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0284 - acc: 0.9922 - val_loss: 0.9758 - val_acc: 0.9068\n",
            "Epoch 39/99\n",
            "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0324 - acc: 0.9908 - val_loss: 1.0497 - val_acc: 0.9135\n",
            "Epoch 40/99\n",
            "60000/60000 [==============================] - 11s 185us/sample - loss: 0.0298 - acc: 0.9919 - val_loss: 1.0340 - val_acc: 0.9114\n",
            "Epoch 41/99\n",
            "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0326 - acc: 0.9913 - val_loss: 1.0707 - val_acc: 0.9150\n",
            "Epoch 42/99\n",
            "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0334 - acc: 0.9907 - val_loss: 1.0378 - val_acc: 0.9168\n",
            "Epoch 43/99\n",
            "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0263 - acc: 0.9925 - val_loss: 1.0012 - val_acc: 0.9095\n",
            "Epoch 44/99\n",
            "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0338 - acc: 0.9911 - val_loss: 1.1826 - val_acc: 0.9143\n",
            "Epoch 45/99\n",
            "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0331 - acc: 0.9922 - val_loss: 1.1655 - val_acc: 0.9114\n",
            "Epoch 46/99\n",
            "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0320 - acc: 0.9920 - val_loss: 1.2023 - val_acc: 0.9164\n",
            "Epoch 47/99\n",
            "60000/60000 [==============================] - 11s 184us/sample - loss: 0.0347 - acc: 0.9915 - val_loss: 1.2012 - val_acc: 0.9164\n",
            "Epoch 48/99\n",
            "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0329 - acc: 0.9920 - val_loss: 1.1987 - val_acc: 0.9146\n",
            "Epoch 49/99\n",
            "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0360 - acc: 0.9909 - val_loss: 1.1840 - val_acc: 0.9141\n",
            "Epoch 50/99\n",
            "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0261 - acc: 0.9933 - val_loss: 1.3798 - val_acc: 0.9166\n",
            "Epoch 51/99\n",
            "60000/60000 [==============================] - 11s 181us/sample - loss: 0.0346 - acc: 0.9922 - val_loss: 1.2920 - val_acc: 0.9135\n",
            "Epoch 52/99\n",
            "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0366 - acc: 0.9915 - val_loss: 1.1666 - val_acc: 0.9157\n",
            "Epoch 53/99\n",
            "60000/60000 [==============================] - 11s 181us/sample - loss: 0.0271 - acc: 0.9928 - val_loss: 1.3619 - val_acc: 0.9156\n",
            "Epoch 54/99\n",
            "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0348 - acc: 0.9924 - val_loss: 1.4101 - val_acc: 0.9094\n",
            "Epoch 55/99\n",
            "60000/60000 [==============================] - 11s 185us/sample - loss: 0.0364 - acc: 0.9916 - val_loss: 1.2904 - val_acc: 0.9113\n",
            "Epoch 56/99\n",
            "60000/60000 [==============================] - 11s 181us/sample - loss: 0.0311 - acc: 0.9926 - val_loss: 1.1975 - val_acc: 0.9138\n",
            "Epoch 57/99\n",
            "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0377 - acc: 0.9917 - val_loss: 1.2486 - val_acc: 0.9119\n",
            "Epoch 58/99\n",
            "60000/60000 [==============================] - 11s 182us/sample - loss: 0.0266 - acc: 0.9939 - val_loss: 1.5104 - val_acc: 0.9117\n",
            "Epoch 59/99\n",
            "60000/60000 [==============================] - 11s 183us/sample - loss: 0.0334 - acc: 0.9926 - val_loss: 1.2469 - val_acc: 0.9088\n",
            "Epoch 60/99\n",
            "60000/60000 [==============================] - 11s 187us/sample - loss: 0.0369 - acc: 0.9920 - val_loss: 1.3688 - val_acc: 0.9092\n",
            "Epoch 00060: early stopping\n",
            "----- test_loss\n",
            "10000/10000 [==============================] - 1s 93us/sample - loss: 1.3688 - acc: 0.9092\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRLfZ0jt-fQI",
        "colab_type": "text"
      },
      "source": [
        "It's likely gone up to about 93% on the training data and 91% on the validation data. \n",
        "\n",
        "That's significant, and a step in the right direction!\n",
        "\n",
        "Try running it for more epochs -- say about 20, and explore the results! But while the results might seem really good, the validation results may actually go down, due to something called 'overfitting' which will be discussed later. \n",
        "\n",
        "(In a nutshell, 'overfitting' occurs when the network learns the data from the training set really well, but it's too specialised to only that data, and as a result is less effective at seeing *other* data. For example, if all your life you only saw red shoes, then when you see a red shoe you would be very good at identifying it, but blue suade shoes might confuse you...and you know you should never mess with my blue suede shoes.)\n",
        "\n",
        "Then, look at the code again, and see, step by step how the Convolutions were built:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaLX5cgI_JDb",
        "colab_type": "text"
      },
      "source": [
        "Step 1 is to gather the data. You'll notice that there's a bit of a change here in that the training data needed to be reshaped. That's because the first convolution expects a single tensor containing everything, so instead of 60,000 28x28x1 items in a list, we have a single 4D list that is 60,000x28x28x1, and the same for the test images. If you don't do this, you'll get an error when training as the Convolutions do not recognize the shape. \n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "import tensorflow as tf\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SS_W_INc_kJQ",
        "colab_type": "text"
      },
      "source": [
        "Next is to define your model. Now instead of the input layer at the top, you're going to add a Convolution. The parameters are:\n",
        "\n",
        "1. The number of convolutions you want to generate. Purely arbitrary, but good to start with something in the order of 32\n",
        "2. The size of the Convolution, in this case a 3x3 grid\n",
        "3. The activation function to use -- in this case we'll use relu, which you might recall is the equivalent of returning x when x>0, else returning 0\n",
        "4. In the first layer, the shape of the input data.\n",
        "\n",
        "You'll follow the Convolution with a MaxPooling layer which is then designed to compress the image, while maintaining the content of the features that were highlighted by the convlution. By specifying (2,2) for the MaxPooling, the effect is to quarter the size of the image. Without going into too much detail here, the idea is that it creates a 2x2 array of pixels, and picks the biggest one, thus turning 4 pixels into 1. It repeats this across the image, and in so doing halves the number of horizontal, and halves the number of vertical pixels, effectively reducing the image by 25%.\n",
        "\n",
        "You can call model.summary() to see the size and shape of the network, and you'll notice that after every MaxPooling layer, the image size is reduced in this way. \n",
        "\n",
        "\n",
        "```\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMorM6daADjA",
        "colab_type": "text"
      },
      "source": [
        "Add another convolution\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2,2)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b1-x-kZF4_tC"
      },
      "source": [
        "Now flatten the output. After this you'll just have the same DNN structure as the non convolutional version\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Flatten(),\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPtqR23uASjX",
        "colab_type": "text"
      },
      "source": [
        "The same 128 dense layers, and 10 output layers as in the pre-convolution example:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0GSsjUhAaSj",
        "colab_type": "text"
      },
      "source": [
        "Now compile the model, call the fit method to do the training, and evaluate the loss and accuracy from the test set.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print(test_acc)\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXx_LX3SAlFs",
        "colab_type": "text"
      },
      "source": [
        "# Visualizing the Convolutions and Pooling\n",
        "\n",
        "This code will show us the convolutions graphically. The print (test_labels[;100]) shows us the first 100 labels in the test set, and you can see that the ones at index 0, index 23 and index 28 are all the same value (9). They're all shoes. Let's take a look at the result of running the convolution on each, and you'll begin to see common features between them emerge. Now, when the DNN is training on that data, it's working with a lot less, and it's perhaps finding a commonality between shoes based on this convolution/pooling combination."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-6nX4QsOku6",
        "colab_type": "code",
        "outputId": "bed2c818-86ff-4c3a-9571-c015ca44eb37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "print(test_labels[:100].reshape((10,10)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[9 2 1 1 6 1 4 6 5 7]\n",
            " [4 5 7 3 4 1 2 4 8 0]\n",
            " [2 5 7 9 1 4 6 0 9 3]\n",
            " [8 8 3 3 8 0 7 5 7 9]\n",
            " [6 1 3 7 6 7 2 1 2 2]\n",
            " [4 4 5 8 2 2 8 4 8 0]\n",
            " [7 7 8 5 1 1 2 3 9 8]\n",
            " [7 0 2 6 2 3 1 2 8 4]\n",
            " [1 8 5 9 5 0 3 2 0 6]\n",
            " [5 3 6 7 1 8 0 1 4 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FGsHhv6JvDx",
        "colab_type": "code",
        "outputId": "0d4be668-539d-4733-f517-d36229241b85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f, axarr = plt.subplots(3,5)\n",
        "\n",
        "### = 0\n",
        "FIRST_IMAGE=19\n",
        "SECOND_IMAGE=27\n",
        "THIRD_IMAGE=35\n",
        "\n",
        "# ### = 1\n",
        "# FIRST_IMAGE=2\n",
        "# SECOND_IMAGE=3\n",
        "# THIRD_IMAGE=5\n",
        "\n",
        "### = 9\n",
        "# FIRST_IMAGE=0\n",
        "# SECOND_IMAGE=28\n",
        "# THIRD_IMAGE=39\n",
        "\n",
        "print( test_labels[FIRST_IMAGE], test_labels[SECOND_IMAGE], test_labels[THIRD_IMAGE] )\n",
        "CONVOLUTION_NUMBER = 1\n",
        "from tensorflow.keras import models\n",
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
        "\n",
        "for x in range(0,4):\n",
        "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[0,x].grid(False)\n",
        "  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[1,x].grid(False)\n",
        "  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[2,x].grid(False)\n",
        "\n",
        "axarr[0,4].imshow(test_images[FIRST_IMAGE].reshape((28,28)))\n",
        "axarr[1,4].imshow(test_images[SECOND_IMAGE].reshape((28,28)))\n",
        "axarr[2,4].imshow(test_images[THIRD_IMAGE].reshape((28,28)))"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f66012ee5c0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD1CAYAAABN5n7dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+f/vnSm84e07HgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 15 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQoBV50i8147",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "6a72ff9d-c221-4a1c-d3d8-a695b5435903"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "f, axarr = plt.subplots(3,5)\n",
        "\n",
        "### = 0\n",
        "FIRST_IMAGE=19\n",
        "SECOND_IMAGE=27\n",
        "THIRD_IMAGE=35\n",
        "\n",
        "# ### = 1\n",
        "# FIRST_IMAGE=2\n",
        "# SECOND_IMAGE=3\n",
        "# THIRD_IMAGE=5\n",
        "\n",
        "### = 9\n",
        "# FIRST_IMAGE=0\n",
        "# SECOND_IMAGE=28\n",
        "# THIRD_IMAGE=39\n",
        "\n",
        "print( test_labels[FIRST_IMAGE], test_labels[SECOND_IMAGE], test_labels[THIRD_IMAGE] )\n",
        "CONVOLUTION_NUMBER = 1\n",
        "from tensorflow.keras import models\n",
        "layer_outputs = [layer.output for layer in model.layers]\n",
        "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
        "\n",
        "for x in range(0,4):\n",
        "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[0,x].grid(False)\n",
        "  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[1,x].grid(False)\n",
        "  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
        "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
        "  axarr[2,x].grid(False)\n",
        "\n",
        "axarr[0,4].imshow(test_images[FIRST_IMAGE].reshape((28,28)))\n",
        "axarr[1,4].imshow(test_images[SECOND_IMAGE].reshape((28,28)))\n",
        "axarr[2,4].imshow(test_images[THIRD_IMAGE].reshape((28,28)))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f66340996a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD1CAYAAABN5n7dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 15 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KVPZqgHo5Ux",
        "colab_type": "text"
      },
      "source": [
        "EXERCISES\n",
        "\n",
        "1. Try editing the convolutions. Change the 32s to either 16 or 64. What impact will this have on accuracy and/or training time.\n",
        "\n",
        "2. Remove the final Convolution. What impact will this have on accuracy or training time?\n",
        "\n",
        "3. How about adding more Convolutions? What impact do you think this will have? Experiment with it.\n",
        "\n",
        "4. Remove all Convolutions but the first. What impact do you think this will have? Experiment with it. \n",
        "\n",
        "5. In the previous lesson you implemented a callback to check on the loss function and to cancel training once it hit a certain amount. See if you can implement that here!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZpYRidBXpBPM",
        "colab_type": "code",
        "outputId": "70253c13-f6e9-4c19-a737-747f557bac51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images.reshape(60000, 28, 28, 1)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 28, 28, 1)\n",
        "test_images=test_images/255.0\n",
        "\n",
        "class stopOnPercentage(tf.keras.callbacks.Callback):\n",
        "  def __init__(self, acc=0.99):\n",
        "    super().__init__()\n",
        "    self.min_acc = acc\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs):\n",
        "    if logs.get('acc') > self.min_acc:\n",
        "      self.model.stop_training = True\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(64, (5,5), activation='relu', input_shape=(28, 28, 1)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(\n",
        "    training_images, training_labels, \n",
        "    validation_data=(test_images, test_labels),\n",
        "    epochs=10,\n",
        "    callbacks = [\n",
        "      stopOnPercentage(.9999)\n",
        "    ])\n",
        "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
        "print('----- test_loss')\n",
        "test_loss = model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.0\n",
            "Model: \"sequential_45\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_85 (Conv2D)           (None, 24, 24, 64)        1664      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_72 (MaxPooling (None, 12, 12, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_45 (Flatten)         (None, 9216)              0         \n",
            "_________________________________________________________________\n",
            "dense_90 (Dense)             (None, 128)               1179776   \n",
            "_________________________________________________________________\n",
            "dense_91 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 1,182,730\n",
            "Trainable params: 1,182,730\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 8s 127us/sample - loss: 0.1162 - acc: 0.9648 - val_loss: 0.0500 - val_acc: 0.9840\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 7s 109us/sample - loss: 0.0410 - acc: 0.9870 - val_loss: 0.0372 - val_acc: 0.9877\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 7s 113us/sample - loss: 0.0249 - acc: 0.9920 - val_loss: 0.0372 - val_acc: 0.9870\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 7s 111us/sample - loss: 0.0174 - acc: 0.9944 - val_loss: 0.0370 - val_acc: 0.9889\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 7s 110us/sample - loss: 0.0122 - acc: 0.9961 - val_loss: 0.0351 - val_acc: 0.9893\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 0.0102 - acc: 0.9966 - val_loss: 0.0483 - val_acc: 0.9876\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 7s 113us/sample - loss: 0.0067 - acc: 0.9978 - val_loss: 0.0448 - val_acc: 0.9890\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 7s 111us/sample - loss: 0.0068 - acc: 0.9977 - val_loss: 0.0390 - val_acc: 0.9905\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 7s 114us/sample - loss: 0.0053 - acc: 0.9981 - val_loss: 0.0429 - val_acc: 0.9899\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 7s 110us/sample - loss: 0.0036 - acc: 0.9989 - val_loss: 0.0606 - val_acc: 0.9885\n",
            "10000/10000 [==============================] - 1s 73us/sample - loss: 0.0606 - acc: 0.9885\n",
            "----- test_loss\n",
            "10000/10000 [==============================] - 1s 73us/sample - loss: 0.0606 - acc: 0.9885\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EtZ92T2OR4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}